{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a063e80c-32ab-4f9a-a1fb-e4ac5569d4d7",
   "metadata": {},
   "source": [
    "# 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984876bf-fd20-48f5-97bc-c18adf7d0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import umap\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826139a-67cb-468e-98aa-713d34fa357f",
   "metadata": {},
   "source": [
    "# 1. Load image embedding from layer activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a932db1-5cab-44d4-93c7-b90e401a95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_img_emb_from_layer_act(model_nickname, layer, dim=None):\n",
    "    if dim is None:\n",
    "        return f'/raid/NeuEvo/data/layer_act/{model_nickname}/data/{layer}/img_emb.txt'\n",
    "    else:\n",
    "        return f'/raid/NeuEvo/data/layer_act/{model_nickname}/data/{layer}/img_emb-dim={dim}.txt'\n",
    "\n",
    "def load_img_emb_from_layer_act(model_nickname, layer, dim):\n",
    "    p = get_path_img_emb_from_layer_act(model_nickname, layer, dim)\n",
    "    data = np.loadtxt(p)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168723e5-c148-426e-8172-5f54cd45d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nickname = 'vgg19_pretrained'\n",
    "layers = [\n",
    "    'Sequential_0_Conv2d_0',\n",
    "    'Sequential_0_Conv2d_2',\n",
    "    'Sequential_0_Conv2d_5',\n",
    "    'Sequential_0_Conv2d_7',\n",
    "    'Sequential_0_Conv2d_10',\n",
    "    'Sequential_0_Conv2d_12',\n",
    "    'Sequential_0_Conv2d_14',\n",
    "    'Sequential_0_Conv2d_16',\n",
    "    'Sequential_0_Conv2d_19',\n",
    "    'Sequential_0_Conv2d_21',\n",
    "    'Sequential_0_Conv2d_23',\n",
    "    'Sequential_0_Conv2d_25',\n",
    "    'Sequential_0_Conv2d_28',\n",
    "    'Sequential_0_Conv2d_30',\n",
    "    'Sequential_0_Conv2d_32',\n",
    "    'Sequential_0_Conv2d_34'\n",
    "]\n",
    "\n",
    "layer_acts = {}\n",
    "with tqdm(total=len(layers)) as pbar:\n",
    "    for layer in layers:\n",
    "        layer_acts[layer] = load_img_emb_from_layer_act(model_nickname, layer, dim=None)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea2e0c-4a08-42f5-9a91-405245fea748",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_num_neurons = {}\n",
    "for layer in layers:\n",
    "    print(f'{layer}: {layer_acts[layer].shape}')\n",
    "    base_num_neurons[layer] = layer_acts[layer].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a731e-c98e-403b-89e2-8a5b55c43249",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = layer_acts[layers[0]].shape[0]\n",
    "print(f'Num imgs: {num_imgs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01674146-ba20-41ca-91e6-c1f7bccdc03a",
   "metadata": {},
   "source": [
    "# 2. Load stimulus and get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e19bd9-e289-412d-af68-4fc7dfefc54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_stimulus_path(model_nickname, topk_s):\n",
    "    return f'/raid/NeuEvo/data/stimulus/{model_nickname}/data/stimulus-topk_s={topk_s}.json'\n",
    "\n",
    "def load_stimulus(model_nickname, topk_s):\n",
    "    p = get_stimulus_path(model_nickname, topk_s)\n",
    "    data = load_json(p)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24261bc-8281-4a03-88a4-225a78a25e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus = {}\n",
    "vocab = {}\n",
    "\n",
    "model_nicknames = [\n",
    "    'vgg19_pretrained',\n",
    "    'convnext_0.004_1',\n",
    "    'convnext_0.004_3',\n",
    "    'convnext_0.004_96'\n",
    "]\n",
    "\n",
    "for model_nickname in model_nicknames:\n",
    "    stimulus[model_nickname] = load_stimulus(model_nickname, 20)\n",
    "    vocab[model_nickname] = {}\n",
    "    for layer in stimulus[model_nickname]:\n",
    "        for neuron_stimuli in stimulus[model_nickname][layer]:\n",
    "            for img in neuron_stimuli:\n",
    "                if img not in vocab[model_nickname]:\n",
    "                    vocab[model_nickname][img] = 0\n",
    "                vocab[model_nickname][img] += 1\n",
    "                \n",
    "for model_nickname in model_nicknames:\n",
    "    print(f'{model_nickname}: {len(vocab[model_nickname])} imgs in the vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f259e-0afd-44db-b490-ac368414c1fb",
   "metadata": {},
   "source": [
    "# 3. Load image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae3a15-3a2a-4e23-85a4-4e1cfb95c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter(Xs, title=None):\n",
    "    X = Xs[:,0]\n",
    "    Y = Xs[:,1]\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(X, Y, s=3, alpha=0.2)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def filter_img_emb_in_vocab(img_emb, vocab, dim):\n",
    "    Xs = np.zeros((len(vocab), dim))\n",
    "    idx2imgidx = {}\n",
    "    for i, img_idx in enumerate(vocab):\n",
    "        Xs[i] = img_emb[img_idx]\n",
    "        idx2imgidx[i] = img_idx\n",
    "    return Xs, idx2imgidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d3ced-d58d-49b3-b3d5-dd39c20bd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "raw_training_datasets = datasets.ImageFolder(\n",
    "    '../../../../ILSVRC2012/train',\n",
    "    raw_data_transform\n",
    ")\n",
    "\n",
    "def show_imgs(img_idxs):\n",
    "    for img_idx in img_idxs:\n",
    "        img = raw_training_datasets[img_idx][0]\n",
    "        img = np.einsum('kij->ijk', img)\n",
    "        plt.title(f'img: {img_idx}')\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "def show_np_imgs_2_rows(imgs, title=None, subplot_titles=None):\n",
    "    # Create figure\n",
    "    nrows = 2\n",
    "    ncols = len(imgs) // nrows\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(2 * ncols, 2 * nrows))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Create subplots\n",
    "    for i, img in enumerate(imgs[:nrows * ncols]):\n",
    "        row = int(i / ncols)\n",
    "        col = i - row * ncols\n",
    "        ax[row, col].imshow(img) \n",
    "        ax[row, col].set_axis_off()\n",
    "        if subplot_titles is not None:\n",
    "            ax[row, col].set_title(subplot_titles[i])\n",
    "\n",
    "    # Title\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, y=1.06)\n",
    "        \n",
    "    # Show the final plot\n",
    "    plt.show()\n",
    "        \n",
    "def show_imgs_2_rows(img_idxs, title=None):\n",
    "    # Images\n",
    "    imgs = []\n",
    "    subplot_titles = []\n",
    "    for i, img_idx in enumerate(img_idxs):\n",
    "        img = raw_training_datasets[img_idx][0]\n",
    "        img = np.einsum('kij->ijk', img)\n",
    "        imgs.append(img)\n",
    "        subplot_titles.append(f'{i}-th img: {img_idx}')\n",
    "        \n",
    "    # Show images\n",
    "    show_np_imgs_2_rows(imgs, title=title, subplot_titles=subplot_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b403c0-e655-4359-8380-0f531984581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image embedding\n",
    "vgg19_img_emb_path = '/raid/NeuEvo/data/embedding/emb-vgg19_pretrained-topk_s=20-dim=30-lr_emb=0.05-num_emb_epochs=10000-num_emb_negs=3/data/emb-set-dim=30-lr_img_emb=10.0-thr_img_emb=0.001-max_iter_img_emb=10000-k=10/emb_nd/img_emb.txt'\n",
    "base_img_emb = np.loadtxt(vgg19_img_emb_path)\n",
    "\n",
    "# Extract embedding of images in vocab\n",
    "dim = 30\n",
    "img_emb_vocab, idx2imgidx = filter_img_emb_in_vocab(\n",
    "    base_img_emb, \n",
    "    vocab['vgg19_pretrained'], \n",
    "    dim\n",
    ")\n",
    "\n",
    "# Reduce image embedding into 2d\n",
    "reducer = umap.UMAP(n_components=2)\n",
    "reducer = reducer.fit(img_emb_vocab)\n",
    "img_emb_vocab_2d = reducer.embedding_\n",
    "\n",
    "# Draw 2D image embedding\n",
    "title='Embedding of images in the vocab (vgg19_pretrained)'\n",
    "draw_scatter(img_emb_vocab_2d, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e155d1f-b014-4a52-ad3c-a6c9a16d35e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img = []\n",
    "for i, vec in enumerate(img_emb_vocab_2d):\n",
    "    x, y = vec[0], vec[1]\n",
    "    if 7.5 <= x <= 8.5 and 3 <= y <= 4:\n",
    "        target_img.append(idx2imgidx[i])\n",
    "\n",
    "show_imgs_2_rows(target_img[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7252fe-2930-4b1b-b4c9-a03355284cd3",
   "metadata": {},
   "source": [
    "# 4. Find image pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f305a58-a369-45f2-9921-b180b17f05ee",
   "metadata": {},
   "source": [
    "## 4-1. Find most activated neurons for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b3477-1b19-44ea-8038-ffcd5c648ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_neurons = {}\n",
    "total = len(layers) * num_imgs\n",
    "with tqdm(total=total) as pbar:\n",
    "    for layer in layers:\n",
    "        sorted_neurons[layer] = {}\n",
    "        for img_i, img_v in enumerate(layer_acts[layer]):\n",
    "            neuron_idxs = np.argsort(-img_v)\n",
    "            neuron_ids = [f'{layer}-{idx}' for idx in neuron_idxs]\n",
    "            sorted_neurons[layer][img_i] = neuron_ids\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a27fb8-a950-4364-8f79-9bfddcd9d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "top_neurons = {}\n",
    "for layer in layers:\n",
    "    top_neurons[layer] = {}\n",
    "    for img_i in range(num_imgs):\n",
    "        top_neurons[layer][img_i] = sorted_neurons[layer][img_i][:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b52bd-a9d4-40a1-876a-3a1d0cfb2c8b",
   "metadata": {},
   "source": [
    "## 4-2. Find images that stimulate many common neurons in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3583e9-e527-4675-81df-a667192dc439",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_stimulating_imgs = {}\n",
    "total = len(layers) * num_imgs\n",
    "with tqdm(total=total) as pbar:\n",
    "    for layer in layers:\n",
    "        for img_i in top_neurons[layer]:\n",
    "            for neuron_id in top_neurons[layer][img_i]:\n",
    "                if neuron_id not in co_stimulating_imgs:\n",
    "                    co_stimulating_imgs[neuron_id] = []\n",
    "                co_stimulating_imgs[neuron_id].append(img_i)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867c6e7-2e65-4d6a-853d-ca9a9c9194c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pairs = {}\n",
    "total = len(co_stimulating_imgs)\n",
    "\n",
    "with tqdm(total=total) as pbar:\n",
    "    for neuron_id in co_stimulating_imgs:\n",
    "        imgs = co_stimulating_imgs[neuron_id]\n",
    "        for i, img_i in enumerate(imgs):\n",
    "            if i == len(imgs) - 1:\n",
    "                break\n",
    "            img_j = imgs[i + 1]\n",
    "            key = '-'.join(list(map(str, sorted([img_i, img_j]))))\n",
    "            if key not in img_pairs:\n",
    "                img_pairs[key] = 0\n",
    "            img_pairs[key] += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "sorted_img_pairs = sorted(img_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f'The number of image pairs: {len(sorted_img_pairs)}')\n",
    "print(f'The number of images: {num_imgs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9b576-fadd-4bc2-ad24-9055c965184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(img_pairs)\n",
    "sim_img_dict = {}\n",
    "with tqdm(total=total) as pbar:\n",
    "    for pair in img_pairs:\n",
    "        img_i, img_j = pair.split('-')\n",
    "        img_i, img_j = int(img_i), int(img_j)\n",
    "        cnt = img_pairs[pair]\n",
    "\n",
    "        if img_i not in sim_img_dict:\n",
    "            sim_img_dict[img_i] = {}\n",
    "        if img_j not in sim_img_dict:\n",
    "            sim_img_dict[img_j] = {}\n",
    "\n",
    "        sim_img_dict[img_i][img_j] = cnt\n",
    "        sim_img_dict[img_j][img_i] = cnt\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7d015-0dd9-42a4-bc8f-ce33bc98090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_img = np.random.choice(num_imgs)\n",
    "similar_imgs = list(sim_img_dict[sampled_img].keys())\n",
    "similar_imgs_in_vocab = [i for i in similar_imgs if i in vocab['vgg19_pretrained']]\n",
    "\n",
    "show_imgs([sampled_img])\n",
    "if len(similar_imgs_in_vocab) > 0:\n",
    "    if 1 <= len(similar_imgs_in_vocab) <= 3:\n",
    "        show_imgs(similar_imgs_in_vocab)\n",
    "    else:\n",
    "        print(len(similar_imgs_in_vocab))\n",
    "        show_imgs_2_rows(similar_imgs_in_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549e3f7-7de5-4d64-961a-6e3072eb70d5",
   "metadata": {},
   "source": [
    "# 5. Project embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f1780-05c8-4985-b37a-54d75c58945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_neurons(model_nickname, stimulus):\n",
    "    n = 0\n",
    "    for layer in stimulus[model_nickname]:\n",
    "        n += len(stimulus[model_nickname][layer])\n",
    "    return n\n",
    "\n",
    "def get_approx_img_vec(img_i, sim_img_dict, img_emb, vocab, dim):\n",
    "    s, v_i = 0, np.zeros(dim)\n",
    "    for img_j in sim_img_dict[img_i]:\n",
    "        if img_j in vocab:\n",
    "            cnt = sim_img_dict[img_i][img_j]\n",
    "            if cnt > 50:\n",
    "                v_i += cnt * img_emb[img_i]\n",
    "                s += cnt\n",
    "\n",
    "    if s <= 0:\n",
    "        return None\n",
    "    \n",
    "    v_i = v_i / s\n",
    "    return v_i\n",
    "\n",
    "def get_approx_neuron_vec(stimulating_imgs, sim_img_dict, img_emb, vocab, dim):\n",
    "    num_valid, v = 0, np.zeros(dim)\n",
    "    for img_i in stimulating_imgs:\n",
    "        if img_i in vocab:\n",
    "            v += img_emb[img_i]\n",
    "            num_valid += 1\n",
    "        else:\n",
    "            v_i = get_approx_img_vec(img_i, sim_img_dict, img_emb, vocab, dim)\n",
    "            if v_i is not None:\n",
    "                v += v_i\n",
    "                num_valid += 1\n",
    "            \n",
    "    if num_valid <= 4:\n",
    "        return None\n",
    "    \n",
    "    v = v / num_valid\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc68129-3e0f-4bf6-98b7-946207a19359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ex_patch_dir_path(model_nickname, topk_s, ex_patch_size_ratio):\n",
    "    root = '/raid/NeuEvo/data/neuron_feature'\n",
    "    return f'{root}/{model_nickname}/data/topk_s={topk_s}-ex_patch_size_ratio={ex_patch_size_ratio}'\n",
    "\n",
    "def get_ex_patch_img_paths(model_nickname, topk_s, ex_patch_size_ratio, layer, neuron_idx):\n",
    "    d = get_ex_patch_dir_path(model_nickname, topk_s, ex_patch_size_ratio)\n",
    "    paths = []\n",
    "    for i in range(topk_s):\n",
    "        paths.append(f'{d}/{layer}-{neuron_idx}-{i}.jpg')\n",
    "    return paths\n",
    "\n",
    "def load_img_from_path(p):\n",
    "    img = cv2.imread(p)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "                     \n",
    "def show_ex_patch(model_nickname, topk_s, ex_patch_size_ratio, layer, neuron_idx):\n",
    "    img_ps = get_ex_patch_img_paths(model_nickname, topk_s, ex_patch_size_ratio, layer, neuron_idx)\n",
    "    imgs = [load_img_from_path(img_path) for img_path in img_ps]\n",
    "    title = f'{model_nickname}, {layer}-{neuron_idx}'\n",
    "    show_np_imgs_2_rows(imgs, title=title, subplot_titles=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f74e0-3146-45fb-bd01-6c009358b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ex_patch_of_region(X, Y, X_2d, idx2id, model_nickname, topk_s=20, k=10):\n",
    "    target = {}\n",
    "    for i, vec in enumerate(X_2d):\n",
    "        x = vec[0]\n",
    "        y = vec[1]\n",
    "        if X[0] <= x <= X[1] and Y[0] <= y <= Y[1]:\n",
    "            target[i] = True\n",
    "\n",
    "    neurons = [idx2id[i] for i in target]\n",
    "\n",
    "    for neuron in neurons[:k]:\n",
    "        layer, neuron_idx = neuron.split('-')\n",
    "        neuron_idx = int(neuron_idx)\n",
    "        show_ex_patch(model_nickname, topk_s, 0.3, layer, neuron_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15518c3c-36a5-4d0a-ac9a-9f6fdd58de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_neuron(stimulus, model_nickname):\n",
    "    layers = list(stimulus[model_nickname].keys())\n",
    "    layer = np.random.choice(layers)\n",
    "    N = len(stimulus[model_nickname][layer])\n",
    "    n = np.random.choice(N)\n",
    "    return layer, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4bbc4-c29b-4679-99c6-aa1f97a96788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reducer\n",
    "neuron_emb_path = '/raid/NeuEvo/data/embedding/emb-vgg19_pretrained-topk_s=20-dim=30-lr_emb=0.05-num_emb_epochs=10000-num_emb_negs=3/data/emb/emb.json'\n",
    "neuron_emb = load_json(neuron_emb_path)\n",
    "N = len(neuron_emb)\n",
    "X_vgg19 = np.zeros((N, dim))\n",
    "idx2id_vgg19 = {}\n",
    "for i, neuron in enumerate(neuron_emb):\n",
    "    X_vgg19[i] = neuron_emb[neuron]\n",
    "    idx2id_vgg19[i] = neuron\n",
    "\n",
    "reducer = umap.UMAP(n_components=2)\n",
    "reducer = reducer.fit(X_vgg19)\n",
    "X_2d_vgg19 = reducer.embedding_\n",
    "\n",
    "draw_scatter(X_2d_vgg19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08920848-e44c-4c42-a372-82e312cb94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show specific region's neurons\n",
    "show_ex_patch_of_region([5, 7], [10, 12], X_2d_vgg19, idx2id_vgg19, basemodel_nickname, topk_s=15, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874087fd-d47b-4a2f-9000-8d98c380a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize approximated embedding\n",
    "model_nickname = 'convnext_0.004_1'\n",
    "basemode_nickname = 'vgg19_pretrained'\n",
    "dim = 30\n",
    "N = get_num_neurons(model_nickname, stimulus)\n",
    "a_X = []\n",
    "\n",
    "# Approximate neuron embeddings\n",
    "n, idx2id = 0, {}\n",
    "with tqdm(total=N) as pbar:\n",
    "    for layer in stimulus[model_nickname]:\n",
    "        for neuron_idx, neuron_stimuli in enumerate(stimulus[model_nickname][layer]):\n",
    "            neuron_id = f'{layer}-{neuron_idx}'\n",
    "            v = get_approx_neuron_vec(neuron_stimuli, sim_img_dict, base_img_emb, vocab[basemode_nickname], dim)\n",
    "            if v is not None:\n",
    "                a_X.append(v)\n",
    "                idx2id[n] = neuron_id\n",
    "                n += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "a_X = np.array(a_X)\n",
    "\n",
    "print(n)\n",
    "print(a_X.shape)\n",
    "\n",
    "# Draw 2D image embedding\n",
    "p_a_X = reducer.transform(a_X)\n",
    "title = f'Projected embedding of {model_nickname}'\n",
    "draw_scatter(p_a_X, title)\n",
    "\n",
    "# Show specific region's neurons\n",
    "show_ex_patch_of_region([5, 7], [10, 12], p_a_X, idx2id, model_nickname, topk_s=20, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e296d-41fa-4511-81c6-fe46f19f37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize approximated embedding\n",
    "model_nickname = 'convnext_0.004_96'\n",
    "basemode_nickname = 'vgg19_pretrained'\n",
    "dim = 30\n",
    "N = get_num_neurons(model_nickname, stimulus)\n",
    "a_X = []\n",
    "\n",
    "# Approximate neuron embeddings\n",
    "n, idx2id = 0, {}\n",
    "with tqdm(total=N) as pbar:\n",
    "    for layer in stimulus[model_nickname]:\n",
    "        for neuron_idx, neuron_stimuli in enumerate(stimulus[model_nickname][layer]):\n",
    "            neuron_id = f'{layer}-{neuron_idx}'\n",
    "            v = get_approx_neuron_vec(neuron_stimuli, sim_img_dict, base_img_emb, vocab[basemode_nickname], dim)\n",
    "            if v is not None:\n",
    "                a_X.append(v)\n",
    "                idx2id[n] = neuron_id\n",
    "                n += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "a_X = np.array(a_X)\n",
    "\n",
    "print(n)\n",
    "print(a_X.shape)\n",
    "\n",
    "# Draw 2D image embedding\n",
    "p_a_X = reducer.transform(a_X)\n",
    "title = f'Projected embedding of {model_nickname}'\n",
    "draw_scatter(p_a_X, title)\n",
    "\n",
    "# Show specific region's neurons\n",
    "show_ex_patch_of_region([5, 7], [10, 12], p_a_X, idx2id, model_nickname, topk_s=20, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb71da4-6c47-4b65-aae0-4a26200b4c78",
   "metadata": {},
   "source": [
    "## 4-2. Find pairs of similar images in the vocab of the base model and our target non-base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77ad7b-6de4-438b-8c8b-645ec5a6f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_img_pairs = {}\n",
    "\n",
    "base_model_nickname = 'vgg19_pretrained'\n",
    "target_model_nicknames = [\n",
    "    # 'convnext_0.004_1',\n",
    "    # 'convnext_0.004_3',\n",
    "    'convnext_0.004_96'\n",
    "]\n",
    "\n",
    "total = np.sum([len(vocab[nickname]) for nickname in target_model_nicknames])\n",
    "with tqdm(total=total) as pbar:\n",
    "    for model_nickname in target_model_nicknames:\n",
    "        for img_i in vocab[model_nickname]:\n",
    "            for img_j in img_pair_dict[img_i]:\n",
    "                key = f'{img_i}-{img_j}'\n",
    "                cnt = img_pair_dict[img_i][img_j]\n",
    "                targeted_img_pairs[key] = cnt\n",
    "            pbar.update(1)\n",
    "            \n",
    "sorted_targeted_img_pairs = sorted(targeted_img_pairs.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c40ca-7379-4df4-97a1-1a55abc8f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "covered_targeted_imgs = {}\n",
    "for pair, cnt in sorted_targeted_img_pairs:\n",
    "    img_i, img_j = pair.split('-')\n",
    "    img_i, img_j = int(img_i), int(img_j)\n",
    "    covered_targeted_imgs[img_i] = True\n",
    "    covered_targeted_imgs[img_j] = True\n",
    "\n",
    "print(f'The number of targeted image pairs: {len(sorted_targeted_img_pairs)}')\n",
    "print(f'The number of images covered by the targeted img pairs: {len(covered_targeted_imgs)}')\n",
    "print(f'The number of images: {num_imgs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9a235-7e49-47ff-8e87-e74f34409856",
   "metadata": {},
   "outputs": [],
   "source": [
    "covered_targeted_imgs = {}\n",
    "for pair, cnt in sorted_targeted_img_pairs:\n",
    "    img_i, img_j = pair.split('-')\n",
    "    img_i, img_j = int(img_i), int(img_j)\n",
    "    covered_targeted_imgs[img_i] = True\n",
    "    covered_targeted_imgs[img_j] = True\n",
    "\n",
    "print(f'The number of targeted image pairs: {len(sorted_targeted_img_pairs)}')\n",
    "print(f'The number of images covered by the targeted img pairs: {len(covered_targeted_imgs)}')\n",
    "print(f'The number of images: {num_imgs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1c1c5-91fe-419f-8867-e78fced2faff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d767a9-cd9a-4272-9468-097a8c29d319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bae2e-2d93-4e28-9188-726ff0214e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d127a4-6add-4f55-80be-6272ab5058eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63c51e44-3c53-4255-aa5b-998e8dda0b94",
   "metadata": {},
   "source": [
    "# 2. For each image, find k most activated neurons "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973d3cf-2b93-4f02-bf3c-0cf209d5de3c",
   "metadata": {},
   "source": [
    "## 2-1. Show selected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f869d4e-3688-4342-908c-755f787f5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "raw_training_datasets = datasets.ImageFolder(\n",
    "    '../../../../ILSVRC2012/train',\n",
    "    raw_data_transform\n",
    ")\n",
    "\n",
    "def show_imgs(img_idxs):\n",
    "    for img_idx in img_idxs:\n",
    "        img = raw_training_datasets[img_idx][0]\n",
    "        img = np.einsum('kij->ijk', img)\n",
    "        plt.title(f'img: {img_idx}')\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "def show_img_pair(imgs, title=None):\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(2 * ncols, 2 * nrows))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    for i, img in enumerate(imgs[:nrows * ncols]):\n",
    "        row = 0\n",
    "        col = i\n",
    "        img_np = raw_training_datasets[img][0]\n",
    "        img_np = np.einsum('kij->ijk', img_np)\n",
    "        ax[col].imshow(img_np) \n",
    "        ax[col].set_axis_off()\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "        \n",
    "def show_np_imgs_2_rows(imgs, title=None, subplot_titles=None):\n",
    "    # Create figure\n",
    "    nrows = 2\n",
    "    ncols = len(imgs) // nrows\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(2 * ncols, 2 * nrows))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Create subplots\n",
    "    for i, img in enumerate(imgs[:nrows * ncols]):\n",
    "        row = int(i / ncols)\n",
    "        col = i - row * ncols\n",
    "        ax[row, col].imshow(img) \n",
    "        ax[row, col].set_axis_off()\n",
    "        if subplot_titles is not None:\n",
    "            ax[row, col].set_title(subplot_titles[i])\n",
    "\n",
    "    # Title\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, y=1.06)\n",
    "        \n",
    "    # Show the final plot\n",
    "    plt.show()\n",
    "        \n",
    "def show_imgs_2_rows(img_idxs, title=None):\n",
    "    # Images\n",
    "    imgs = []\n",
    "    subplot_titles = []\n",
    "    for i, img_idx in enumerate(img_idxs):\n",
    "        img = raw_training_datasets[img_idx][0]\n",
    "        img = np.einsum('kij->ijk', img)\n",
    "        imgs.append(img)\n",
    "        subplot_titles.append(f'{i}-th img: {img_idx}')\n",
    "        \n",
    "    # Show images\n",
    "    show_np_imgs_2_rows(imgs, title=title, subplot_titles=subplot_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85eea2-99ff-4701-8564-c77442510e4b",
   "metadata": {},
   "source": [
    "## 2-2. Find most activated neurons for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8e233-124e-4123-8dd3-82e4f3baeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_neurons = {}\n",
    "total = len(layers) * num_imgs\n",
    "with tqdm(total=total) as pbar:\n",
    "    for layer in layers:\n",
    "        sorted_neurons[layer] = {}\n",
    "        for img_i, img_v in enumerate(layer_acts[layer]):\n",
    "            neuron_idxs = np.argsort(-img_v)\n",
    "            neuron_ids = [f'{layer}-{idx}' for idx in neuron_idxs]\n",
    "            sorted_neurons[layer][img_i] = neuron_ids\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08be3f8-5d74-47b5-a015-1e933ea45e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "top_neurons = {}\n",
    "for layer in layers:\n",
    "    top_neurons[layer] = {}\n",
    "    for img_i in range(num_imgs):\n",
    "        top_neurons[layer][img_i] = sorted_neurons[layer][img_i][:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a3524-9da0-4e6c-8edd-f3b4d48e92d8",
   "metadata": {},
   "source": [
    "## 2-3. Find pairs of images that frequently stimulate the same neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a2262-7c4f-4cab-b0dc-c378ca8b3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_stimulating_imgs = {}\n",
    "total = len(layers) * num_imgs\n",
    "with tqdm(total=total) as pbar:\n",
    "    for layer in layers:\n",
    "        for img_i in top_neurons[layer]:\n",
    "            for neuron_id in top_neurons[layer][img_i]:\n",
    "                if neuron_id not in co_stimulating_imgs:\n",
    "                    co_stimulating_imgs[neuron_id] = []\n",
    "                co_stimulating_imgs[neuron_id].append(img_i)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79945dc5-8692-49cc-981d-77540e0eab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pairs = {}\n",
    "total = len(co_stimulating_imgs)\n",
    "with tqdm(total=total) as pbar:\n",
    "    for neuron_id in co_stimulating_imgs:\n",
    "        imgs = co_stimulating_imgs[neuron_id]\n",
    "        for i, img_i in enumerate(imgs):\n",
    "            if i == len(imgs) - 1:\n",
    "                break\n",
    "            img_j = imgs[i + 1]\n",
    "            key = '-'.join(list(map(str, sorted([img_i, img_j]))))\n",
    "            if key not in img_pairs:\n",
    "                img_pairs[key] = 0\n",
    "            img_pairs[key] += 1\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e4e7f2-2b05-4801-a07e-cfe4484ba1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_img_pairs = sorted(img_pairs.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79c70d-e54c-4222-a5fe-d6045ef11b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of image pairs: {len(sorted_img_pairs)}')\n",
    "print(f'The number of images: {num_imgs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d97f03-3f1b-4588-bed8-cba568e8639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair, cnt in sorted_img_pairs[0:5]:\n",
    "    img_i, img_j = pair.split('-')\n",
    "    img_i, img_j = int(img_i), int(img_j)\n",
    "    show_img_pair([img_i, img_j], title=f'Images ({img_i}, {img_j}), cnt={cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70dfb8-799c-4948-9b44-7f68372a41b4",
   "metadata": {},
   "source": [
    "# 3. Learn embedding of images, that are not in the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7faa5-e2ec-49ab-a729-51bfcac67d9f",
   "metadata": {},
   "source": [
    "## 3-1. Load stimulus and get the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d33581-b183-4359-8a89-6ba350679c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_stimulus_path(model_nickname, topk_s):\n",
    "    return f'/raid/NeuEvo/data/stimulus/{model_nickname}/data/stimulus-topk_s={topk_s}.json'\n",
    "\n",
    "def load_stimulus(model_nickname, topk_s):\n",
    "    p = get_stimulus_path(model_nickname, topk_s)\n",
    "    data = load_json(p)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94190ec3-206e-4405-ac5b-77b8cd6429ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_stimulus = load_stimulus('vgg19_pretrained', 20)\n",
    "vocab = {}\n",
    "for layer in vgg19_stimulus:\n",
    "    for neuron_stimuli in vgg19_stimulus[layer]:\n",
    "        for img in neuron_stimuli:\n",
    "            if img not in vocab:\n",
    "                vocab[img] = 0\n",
    "            vocab[img] += 1\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47b11e-bf2d-43e1-88fc-9cff454a49c8",
   "metadata": {},
   "source": [
    "## 3-2. Load image embedding of the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2d9d4-a437-49e5-b2cf-d4df3f8b1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter(Xs):\n",
    "    X = Xs[:,0]\n",
    "    Y = Xs[:,1]\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(X, Y, s=3, alpha=0.2)\n",
    "    plt.show()\n",
    "    \n",
    "def show_ex_patch_of_region(X, Y, X_2d, idx2id, model_nickname, topk_s=20, k=10):\n",
    "    target = {}\n",
    "    for i, vec in enumerate(X_2d):\n",
    "        x = vec[0]\n",
    "        y = vec[1]\n",
    "        if X[0] <= x <= X[1] and Y[0] <= y <= Y[1]:\n",
    "            target[i] = True\n",
    "\n",
    "    neurons = [idx2id[i] for i in target]\n",
    "\n",
    "    for neuron in neurons[:k]:\n",
    "        layer, neuron_idx = neuron.split('-')\n",
    "        neuron_idx = int(neuron_idx)\n",
    "        show_ex_patch(model_nickname, topk_s, 0.3, layer, neuron_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d3d1d-4b20-4eb2-b3ae-4b973323df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_img_emb_path = '/raid/NeuEvo/data/embedding/emb-vgg19_pretrained-topk_s=20-dim=30-lr_emb=0.05-num_emb_epochs=10000-num_emb_negs=3/data/emb-set-dim=30-lr_img_emb=10.0-thr_img_emb=0.001-max_iter_img_emb=10000-k=10/emb_nd/img_emb.txt'\n",
    "base_img_emb = np.loadtxt(vgg19_img_emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368e838-93ac-4115-8781-a49fccd75d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep copy of base_img_emb\n",
    "img_emb = np.array(base_img_emb)\n",
    "img_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1de30c-a24d-4b28-9bf0-46c2e3273d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction of the base model\n",
    "neuron_emb_path = '/raid/NeuEvo/data/embedding/emb-vgg19_pretrained-topk_s=20-dim=30-lr_emb=0.05-num_emb_epochs=10000-num_emb_negs=3/data/emb/emb.json'\n",
    "neuron_emb = load_json(neuron_emb_path)\n",
    "N = len(neuron_emb)\n",
    "X_vgg19 = np.zeros((N, dim))\n",
    "idx2id_vgg19 = {}\n",
    "for i, neuron in enumerate(neuron_emb):\n",
    "    X_vgg19[i] = neuron_emb[neuron]\n",
    "    idx2id_vgg19[i] = neuron\n",
    "\n",
    "reducer = umap.UMAP(n_components=2)\n",
    "reducer = reducer.fit(X_vgg19)\n",
    "X_2d_vgg19 = reducer.embedding_\n",
    "\n",
    "draw_scatter(X_2d_vgg19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fe857-d72e-4367-9462-6ad451276e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ex_patch_of_region([5, 7], [11, 12], X_2d_vgg19, idx2id_vgg19, 'vgg19_pretrained', 15, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb232eec-0bd2-44b8-bfbb-9c584d67ed20",
   "metadata": {},
   "source": [
    "## 3-3. Learn image embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba2b3f-c256-47e1-a763-126160fe6c53",
   "metadata": {},
   "source": [
    "### 3-3-1. Before learning image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281102b-5dc9-420b-a91d-484291bd6523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_avg_img_vec(imgs, i_X, dim):\n",
    "    avg_v = np.zeros(dim)\n",
    "    for img in imgs:\n",
    "        avg_v += i_X[img]\n",
    "    avg_v /= len(imgs)\n",
    "    return avg_v\n",
    "    \n",
    "def project_vector(model_nickname, i_X, vocab):\n",
    "    # Get stimulus\n",
    "    stimulus = load_stimulus(model_nickname, 20)\n",
    "    \n",
    "    # Get number of neurons\n",
    "    num_neurons = 0\n",
    "    for layer in stimulus:\n",
    "        num_neurons += len(stimulus[layer])\n",
    "        \n",
    "    # Get projection vector\n",
    "    dim = 30\n",
    "    p_X = np.zeros((num_neurons, dim))\n",
    "    n = 0\n",
    "    for layer in stimulus:\n",
    "        for neuron_stimuli in stimulus[layer]:\n",
    "            # imgs = [img for img in neuron_stimuli if img in vocab]\n",
    "            # if len(imgs) > 0:\n",
    "            #     p_X[n] = gen_avg_img_vec(imgs, i_X, dim)\n",
    "            p_X[n] = gen_avg_img_vec(neuron_stimuli, i_X, dim)\n",
    "            n += 1\n",
    "    return p_X\n",
    "\n",
    "def project_vector_with_mapping(model_nickname, i_X, vocab):\n",
    "    # Get stimulus\n",
    "    stimulus = load_stimulus(model_nickname, 20)\n",
    "    \n",
    "    # Get number of neurons\n",
    "    num_neurons = 0\n",
    "    for layer in stimulus:\n",
    "        num_neurons += len(stimulus[layer])\n",
    "        \n",
    "    # Get projection vector\n",
    "    dim = 30\n",
    "    p_X = np.zeros((num_neurons, dim))\n",
    "    n = 0\n",
    "    idx2id = {}\n",
    "    for layer in stimulus:\n",
    "        for neuron_idx, neuron_stimuli in enumerate(stimulus[layer]):\n",
    "            imgs = [img for img in neuron_stimuli if img in vocab]\n",
    "            if len(imgs) > 7:\n",
    "                p_X[n] = gen_avg_img_vec(imgs, i_X, dim)\n",
    "                idx2id[n] = f'{layer}-{neuron_idx}'\n",
    "            # p_X[n] = gen_avg_img_vec(neuron_stimuli, i_X, dim)\n",
    "            # idx2id[n] = f'{layer}-{neuron_idx}'\n",
    "            n += 1\n",
    "    return p_X, idx2id\n",
    "\n",
    "def draw_projection(model_nickname, i_X, vocab):\n",
    "    p_X = project_vector(model_nickname, i_X, vocab)\n",
    "    p_X_2d = reduce_dim(p_X)\n",
    "    draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94de2c-42c3-45f4-8cfe-5485e5df86e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_0', base_img_emb, vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)\n",
    "show_ex_patch_of_region([1, 3], [4.5, 5.5], p_X_2d_convnext, idx2id_convnext, 'convnext_0.004_0', 20, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51f756-7025-47d1-a25f-58c3fa35cf2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_96', base_img_emb, vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)\n",
    "show_ex_patch_of_region([1, 3], [4.5, 5.5], p_X_2d_convnext, idx2id_convnext, 'convnext_0.004_96', 20, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f9db3-f42c-4bfc-99d3-485dffb85b0c",
   "metadata": {},
   "source": [
    "### 3-3-2. After learning image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41528247-91ae-4dec-9861-b4ab8a6d38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sample_rand_imgs(num_imgs, num):\n",
    "    return np.random.choice(num_imgs, num, replace=False)\n",
    "\n",
    "def sample_img_pair(img_pairs, num_samples):\n",
    "    pairs = np.array(list(img_pairs.keys()))\n",
    "    cnts = np.array(list(img_pairs.values()))\n",
    "    probs = cnts / np.sum(cnts)\n",
    "    sampled_pairs = np.random.choice(pairs, size=num_samples, p=probs)\n",
    "    return sampled_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55decf8f-32b2-4e51-a15a-c8006fbeb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_image_embedding(vocab, added_vocab, img_emb, img_pairs, kwargs):\n",
    "    # Hyperparameters\n",
    "    num_epochs = kwargs['num_epochs']\n",
    "    num_rep_epochs = kwargs['num_rep_epochs']\n",
    "    num_sampled_pairs = kwargs['num_sampled_pairs']\n",
    "    lr = kwargs['lr']\n",
    "    R = kwargs['R']\n",
    "    \n",
    "    # Learn image embedding\n",
    "    total = num_epochs * num_sampled_pairs * num_rep_epochs\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for epoch in range(num_epochs):\n",
    "            sampled_img_pairs = sample_img_pair(img_pairs, num_sampled_pairs)\n",
    "            for rep_epoch in range(num_rep_epochs):\n",
    "                np.random.shuffle(sampled_img_pairs)\n",
    "                for pair in sampled_img_pairs:\n",
    "                    img_i, img_j = pair.split('-')\n",
    "                    img_i, img_j = int(img_i), int(img_j)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    if (img_i not in vocab) and (img_j not in vocab):\n",
    "                        continue\n",
    "\n",
    "                    # Get image vectors\n",
    "                    v_i = img_emb[img_i]\n",
    "                    v_j = img_emb[img_j]\n",
    "                    coeff = 1 - sigmoid(v_i.dot(v_j))\n",
    "\n",
    "                    # Update gradients for v_i\n",
    "                    if img_i not in vocab:\n",
    "                        g_i = coeff * v_j\n",
    "                        rand_imgs = sample_rand_imgs(num_imgs, R)\n",
    "                        for img_r in rand_imgs:\n",
    "                            v_r = img_emb[img_r]\n",
    "                            g_i -= sigmoid(v_i.dot(v_r)) * v_r\n",
    "                        img_emb[img_i] += lr * g_i\n",
    "                        if img_i not in added_vocab:\n",
    "                            added_vocab[img_i] = 0\n",
    "                        added_vocab[img_i] += 1\n",
    "\n",
    "                    # Update gradients for v_j\n",
    "                    if img_j not in vocab:\n",
    "                        g_j = coeff * v_i\n",
    "                        rand_imgs = sample_rand_imgs(num_imgs, R)\n",
    "                        for img_r in rand_imgs:\n",
    "                            v_r = img_emb[img_r]\n",
    "                            g_j -= sigmoid(v_j.dot(v_r)) * v_r\n",
    "                        img_emb[img_j] += lr * g_j\n",
    "\n",
    "                        if img_j not in added_vocab:\n",
    "                            added_vocab[img_j] = 0\n",
    "                        added_vocab[img_j] += 1\n",
    "    return added_vocab, img_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85a5f3-32f0-4981-b829-b9a859e0a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'num_epochs': 2000,\n",
    "    'num_rep_epochs': 5,\n",
    "    'num_sampled_pairs': 2000,\n",
    "    'lr': 0.5,\n",
    "    'R': 3\n",
    "    # 'R': 0\n",
    "}\n",
    "added_vocab, img_emb = learn_image_embedding(vocab, added_vocab, img_emb, img_pairs, kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6d1c4-390c-4142-9088-c9e48932cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {}\n",
    "for img in vocab:\n",
    "    plus = 0\n",
    "    if img in added_vocab:\n",
    "        plus = added_vocab[img]\n",
    "    new_vocab[img] = vocab[img] + plus\n",
    "    \n",
    "for img in added_vocab:\n",
    "    if img not in vocab:\n",
    "        new_vocab[img] = added_vocab[img]\n",
    "        \n",
    "print(len(vocab))\n",
    "print(len(added_vocab))\n",
    "print(len(new_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe8069-69d4-48ab-8b24-2d70961681cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_0', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085764e-9234-4477-872e-9f2d11e4bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_96', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ee9b2-3214-4f41-bf4b-3d1493babd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ex_patch_of_region([4, 8], [10, 12], p_X_2d_convnext, idx2id_convnext, 'convnext_0.004_96', 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb23c6-b138-4cf3-bb10-e5910896c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.02_17', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3fb9e7-03e8-4f98-9c13-bbd0ef795707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6ad59-44ee-4ad1-aa36-e4063056264c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3de3f-ed8d-4b25-9e71-792ea170794a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd8625-0f68-4454-9184-0dae146c6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_96', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f03b4-442c-443c-9112-f9f12ee4819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {}\n",
    "for img in vocab:\n",
    "    plus = 0\n",
    "    if img in added_vocab:\n",
    "        plus = added_vocab[img]\n",
    "    new_vocab[img] = vocab[img] + plus\n",
    "    \n",
    "for img in added_vocab:\n",
    "    if img not in vocab:\n",
    "        new_vocab[img] = added_vocab[img]\n",
    "        \n",
    "print(len(vocab))\n",
    "print(len(added_vocab))\n",
    "print(len(new_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ed373-8e68-4245-bba2-e8fa1190824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_96', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa446d-5c7b-4d56-95d7-69c6f4fbab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {}\n",
    "for img in vocab:\n",
    "    plus = 0\n",
    "    if img in added_vocab:\n",
    "        plus = added_vocab[img]\n",
    "    new_vocab[img] = vocab[img] + plus\n",
    "    \n",
    "for img in added_vocab:\n",
    "    if img not in vocab:\n",
    "        new_vocab[img] = added_vocab[img]\n",
    "        \n",
    "print(len(vocab))\n",
    "print(len(added_vocab))\n",
    "print(len(new_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71026350-ac83-4126-aaa1-02a8600589a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_96', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535fc09d-86de-4b44-ac5d-c639c7e68f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_0', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cad83-f5a0-457f-b659-e3971c31bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_0', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ae117-53a6-4f43-b00b-abbe7a5d5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_96', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e0220-ea47-445b-b9b2-26a3e28ce6ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_ex_patch_of_region([5, 7], [11, 12], p_X_2d_convnext, idx2id_convnext, 'convnext_0.004_96', 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c9f08-780b-4b03-b0c8-a41e1bd2f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_96', img_emb, vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)\n",
    "# show_ex_patch_of_region([1, 3], [4.5, 5.5], p_X_2d_convnext, idx2id_convnext, 'convnext_0.004_96', 20, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a57caf-24f9-4205-8c71-9f7f1685aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_0', img_emb, vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e40319-dc30-49e2-943c-f53c2eb3f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_0', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19a4b3-3ae3-41fc-9c30-811635dbe383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_0', img_emb, new_vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e16532-dcd3-4a40-824a-c06f3a0a0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_96', img_emb, vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb322d8-8c67-4c42-8b77-99fd3bc549c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After num_epochs = 100, num_sampled_pairs = 100, ~5 mins\n",
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_96', img_emb, vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77772c7-fdd9-4929-ab15-acbc60757770",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ex_patch_of_region([1, 3], [4.5, 5.5], p_X_2d_convnext, idx2id_convnext, 'convnext_0.004_96', 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b450644-b526-49e4-89aa-2580ec0badb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_avg_img_vec(imgs, i_X, dim):\n",
    "    avg_v = np.zeros(dim)\n",
    "    for img in imgs:\n",
    "        avg_v += i_X[img]\n",
    "    avg_v /= len(imgs)\n",
    "    return avg_v\n",
    "\n",
    "\n",
    "    \n",
    "def reduce_dim(X):\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    reducer = reducer.fit(X)\n",
    "    return reducer.embedding_\n",
    "\n",
    "def project_vector(model_nickname, i_X, vocab):\n",
    "    # Get stimulus\n",
    "    stimulus = load_stimulus(model_nickname, 20)\n",
    "    \n",
    "    # Get number of neurons\n",
    "    num_neurons = 0\n",
    "    for layer in stimulus:\n",
    "        num_neurons += len(stimulus[layer])\n",
    "        \n",
    "    # Get projection vector\n",
    "    dim = 30\n",
    "    p_X = np.zeros((num_neurons, dim))\n",
    "    n = 0\n",
    "    for layer in stimulus:\n",
    "        for neuron_stimuli in stimulus[layer]:\n",
    "            imgs = [img for img in neuron_stimuli if img in vocab]\n",
    "            if len(imgs) > 0:\n",
    "                p_X[n] = gen_avg_img_vec(imgs, i_X, dim)\n",
    "            n += 1\n",
    "    return p_X\n",
    "\n",
    "def project_vector_with_mapping(model_nickname, i_X, vocab):\n",
    "    # Get stimulus\n",
    "    stimulus = load_stimulus(model_nickname, 20)\n",
    "    \n",
    "    # Get number of neurons\n",
    "    num_neurons = 0\n",
    "    for layer in stimulus:\n",
    "        num_neurons += len(stimulus[layer])\n",
    "        \n",
    "    # Get projection vector\n",
    "    dim = 30\n",
    "    p_X = np.zeros((num_neurons, dim))\n",
    "    n = 0\n",
    "    idx2id = {}\n",
    "    for layer in stimulus:\n",
    "        for neuron_idx, neuron_stimuli in enumerate(stimulus[layer]):\n",
    "            imgs = [img for img in neuron_stimuli if img in vocab]\n",
    "            if len(imgs) > 0:\n",
    "                p_X[n] = gen_avg_img_vec(imgs, i_X, dim)\n",
    "                idx2id[n] = f'{layer}-{neuron_idx}'\n",
    "            n += 1\n",
    "    return p_X, idx2id\n",
    "\n",
    "def draw_projection(model_nickname, i_X, vocab):\n",
    "    p_X = project_vector(model_nickname, i_X, vocab)\n",
    "    p_X_2d = reduce_dim(p_X)\n",
    "    draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8bede-caa7-4704-87da-0f44513fc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ex_patch_dir_path(model_nickname, topk_s, ex_patch_size_ratio):\n",
    "    root = '/raid/NeuEvo/data/neuron_feature'\n",
    "    return f'{root}/{model_nickname}/data/topk_s={topk_s}-ex_patch_size_ratio={ex_patch_size_ratio}'\n",
    "\n",
    "def get_ex_patch_img_paths(model_nickname, topk_s, ex_patch_size_ratio, layer, neuron_idx):\n",
    "    d = get_ex_patch_dir_path(model_nickname, topk_s, ex_patch_size_ratio)\n",
    "    paths = []\n",
    "    for i in range(topk_s):\n",
    "        paths.append(f'{d}/{layer}-{neuron_idx}-{i}.jpg')\n",
    "    return paths\n",
    "\n",
    "def load_img_from_path(p):\n",
    "    img = cv2.imread(p)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "                     \n",
    "def show_ex_patch(model_nickname, topk_s, ex_patch_size_ratio, layer, neuron_idx):\n",
    "    img_ps = get_ex_patch_img_paths(model_nickname, topk_s, ex_patch_size_ratio, layer, neuron_idx)\n",
    "    imgs = [load_img_from_path(img_path) for img_path in img_ps]\n",
    "    title = f'{model_nickname}, {layer}-{neuron_idx}'\n",
    "    show_np_imgs_2_rows(imgs, title=title, subplot_titles=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e0e92-e7a4-4e06-9561-031f186cdcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_projection('vgg19_pretrained', vgg19_img_emb, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543c66a-67ff-495f-9fdf-d3bc66eabbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_emb_path = '/raid/NeuEvo/data/embedding/emb-vgg19_pretrained-topk_s=20-dim=30-lr_emb=0.05-num_emb_epochs=10000-num_emb_negs=3/data/emb/emb.json'\n",
    "neuron_emb = load_json(neuron_emb_path)\n",
    "N = len(neuron_emb)\n",
    "X_vgg19 = np.zeros((N, dim))\n",
    "for i, neuron in enumerate(neuron_emb):\n",
    "    X_vgg19[i] = neuron_emb[neuron]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bcef2-6d27-44df-85c6-dfcaa0c8d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=2)\n",
    "reducer = reducer.fit(X_vgg19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555075ed-2da1-4d01-b472-e929e0d79c9c",
   "metadata": {},
   "source": [
    "### VGG16_0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e8548-6494-4245-a4c4-b2d09d457391",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('vgg16_0.01_0', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7429d3-974d-49da-ab2d-ebc9e139366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('vgg16_0.01_21', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a7e6db-222a-4391-887b-ecfddb9c35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('vgg16_0.01_207', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea31d8-a62d-4776-afe6-343561e4165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X_vgg16, idx2id_vgg16 = project_vector_with_mapping('vgg16_0.01_207', vgg19_img_emb, vocab)\n",
    "p_X_2d_vgg16 = reducer.transform(p_X_vgg16)\n",
    "draw_scatter(p_X_2d_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfee0ee-8e00-4b10-81bd-00fdcd0e4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {}\n",
    "for i, vec in enumerate(p_X_2d_vgg16):\n",
    "    x = vec[0]\n",
    "    y = vec[1]\n",
    "    if 2 <= x <= 3 and 4.5 <= y <= 5.5:\n",
    "        target[i] = True\n",
    "        \n",
    "neurons = [idx2id_vgg16[i] for i in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23403aa-2871-45af-95ac-702f2ebf8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron in neurons[:10]:\n",
    "    layer, neuron_idx = neuron.split('-')\n",
    "    neuron_idx = int(neuron_idx)\n",
    "    show_ex_patch('vgg16_0.01_207', 20, 0.3, layer, neuron_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be905f-11ad-4e7a-9c20-84b4f863d497",
   "metadata": {},
   "source": [
    "### VGG16_0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5dad98-268b-437c-a11c-b53ac7e38879",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('vgg16_0.05_0', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e2822-b766-431c-9526-fef5565c1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('vgg16_0.05_3', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f6903-a3fd-4fc4-a8b0-2623f8f2990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('vgg16_0.05_12', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd323249-b632-44ec-8348-fd33037a687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('vgg16_0.05_13', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43ea0b-3c0e-4975-9a9c-727dbc7c110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('vgg16_0.05_14', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2f8a5-7b40-4889-bd3d-2823bfcf5efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('vgg16_0.05_54', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db012e-00bd-4187-96d9-7d6f28f66a8a",
   "metadata": {},
   "source": [
    "### ConvNeXt_0.004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818dd23-21f6-4238-ae3b-51e9f0d63b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.004_0', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)\n",
    "# 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19912618-56ec-4550-b8da-a442778fa919",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X_convnext, idx2id_convnext = project_vector_with_mapping('convnext_0.004_0', vgg19_img_emb, vocab)\n",
    "p_X_2d_convnext = reducer.transform(p_X_convnext)\n",
    "draw_scatter(p_X_2d_convnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b5ff4-1cda-4e8a-9691-6212829567a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {}\n",
    "for i, vec in enumerate(p_X_2d_convnext):\n",
    "    x = vec[0]\n",
    "    y = vec[1]\n",
    "    if 1.8 <= x <= 2.2 and 6 <= y <= 7:\n",
    "        target[i] = True\n",
    "        \n",
    "neurons = [idx2id_convnext[i] for i in target]\n",
    "\n",
    "for neuron in neurons[:10]:\n",
    "    layer, neuron_idx = neuron.split('-')\n",
    "    neuron_idx = int(neuron_idx)\n",
    "    show_ex_patch('convnext_0.004_0', 20, 0.3, layer, neuron_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4853b-07dc-44c2-a7b5-bef3e4ccf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.004_1', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)\n",
    "# 36%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548dcd4-c671-4921-8635-648d1e6dc18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.004_3', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)\n",
    "54%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc1b96-7351-46fa-88ff-0b2648c5115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.004_91', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5fada-03e6-400d-80cd-2aaa9bec6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.004_96', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5300be-3a19-4a5d-8f31-03319780a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X_convnext_96, idx2id_convnext_96 = project_vector_with_mapping('convnext_0.004_96', vgg19_img_emb, vocab)\n",
    "p_X_2d_convnext_96 = reducer.transform(p_X_convnext_96)\n",
    "draw_scatter(p_X_2d_convnext_96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9c8ec-14b2-4590-bcc9-a0c9cfa1871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {}\n",
    "for i, vec in enumerate(p_X_2d_convnext_96):\n",
    "    x = vec[0]\n",
    "    y = vec[1]\n",
    "    if 1.8 <= x <= 2.2 and 6 <= y <= 7:\n",
    "        target[i] = True\n",
    "        \n",
    "neurons = [idx2id_convnext_96[i] for i in target]\n",
    "\n",
    "for neuron in neurons[:10]:\n",
    "    layer, neuron_idx = neuron.split('-')\n",
    "    neuron_idx = int(neuron_idx)\n",
    "    show_ex_patch('convnext_0.004_96', 20, 0.3, layer, neuron_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992a989-4924-4497-b3ad-0ea81765d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.004_lambda0_0', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597686c5-9ff2-4907-b421-8d2b0c7d516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.004_1', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e0b92-2317-4f5a-92c7-cf7b1018dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.02_0', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ab10d-1dd1-42dd-94b5-1399f56e0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.02_9', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33803bc3-6fa4-4ca9-a675-07017b4e5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.02_10', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485d2aa-9026-4dda-9022-4623d4193d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.02_13', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef926599-0a0a-4549-ac19-2d8a072171bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.02_14', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271ac9f-2cdb-437e-851f-72105cfa8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.02_17', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427153c8-03a2-4ab4-96cc-99e3e8cdca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_0.02_19', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f03eb9-6efb-437a-b017-079f3acaf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('inception_v3_1.5_0', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5dcdb-978c-4565-a83b-354a20dbabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('inception_v3_1.5_4', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f2e1a-c0d1-4d02-be05-891fc74537ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('inception_v3_1.5_70', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a090d51-8d08-4928-ba29-14ea2a60cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('inception_v3_1.5_71', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfcc1c0-c175-4688-b536-938155cca834",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('inception_v3_1.5_72', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca758c36-48ce-4edb-bee8-a5818091e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('inception_v3_1.5_73', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7ca34-d214-49e7-8dc4-522b2f58dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('inception_v3_1.5_100', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada6e99-2b90-412d-995b-3bf2e0e2f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('vgg19_pretrained', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e9b58-98a7-44b5-b559-421a73b25727",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_X = project_vector('convnext_pretrained', vgg19_img_emb, vocab)\n",
    "p_X_2d = reducer.transform(p_X)\n",
    "draw_scatter(p_X_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddd8f1-ff86-48a6-a9bb-20345c7bcc75",
   "metadata": {},
   "source": [
    "# 2. Can layer activation find a group of similar images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9700568-e4cb-4d49-b9f2-a8173d48f113",
   "metadata": {},
   "source": [
    "## 2-1. Showing training images given image indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c4a62-5e22-4906-9e0c-616376a07242",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "raw_training_datasets = datasets.ImageFolder(\n",
    "    '../../../../ILSVRC2012/train',\n",
    "    raw_data_transform\n",
    ")\n",
    "\n",
    "def show_imgs(img_idxs):\n",
    "    for img_idx in img_idxs:\n",
    "        img = raw_training_datasets[img_idx][0]\n",
    "        img = np.einsum('kij->ijk', img)\n",
    "        plt.title(f'img: {img_idx}')\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "def show_np_imgs_2_rows(imgs, title=None, subplot_titles=None):\n",
    "    # Create figure\n",
    "    nrows = 2\n",
    "    ncols = len(imgs) // nrows\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(2 * ncols, 2 * nrows))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Create subplots\n",
    "    for i, img in enumerate(imgs[:nrows * ncols]):\n",
    "        row = int(i / ncols)\n",
    "        col = i - row * ncols\n",
    "        ax[row, col].imshow(img) \n",
    "        ax[row, col].set_axis_off()\n",
    "        if subplot_titles is not None:\n",
    "            ax[row, col].set_title(subplot_titles[i])\n",
    "\n",
    "    # Title\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, y=1.06)\n",
    "        \n",
    "    # Show the final plot\n",
    "    plt.show()\n",
    "        \n",
    "def show_imgs_2_rows(img_idxs, title=None):\n",
    "    # Images\n",
    "    imgs = []\n",
    "    subplot_titles = []\n",
    "    for i, img_idx in enumerate(img_idxs):\n",
    "        img = raw_training_datasets[img_idx][0]\n",
    "        img = np.einsum('kij->ijk', img)\n",
    "        imgs.append(img)\n",
    "        subplot_titles.append(f'{i}-th img: {img_idx}')\n",
    "        \n",
    "    # Show images\n",
    "    show_np_imgs_2_rows(imgs, title=title, subplot_titles=subplot_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b30168-5ad3-4ca7-91a7-1614ffed3481",
   "metadata": {},
   "source": [
    "## 2-2. Pick an image and find the most similar images based on the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36105c-fc62-42b1-956e-ca511aa78b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v, w):\n",
    "    n_v = np.linalg.norm(v)\n",
    "    n_w = np.linalg.norm(w)\n",
    "    if n_v <= 0 or n_w <= 0:\n",
    "        return 0\n",
    "    return v.dot(w) / (n_v * n_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e3991-5fc9-4e2e-b279-5462a61e281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample an image\n",
    "num_imgs = img_emb.shape[0]\n",
    "sampled_img_idx = np.random.choice(num_imgs)\n",
    "# print(f'Sampled image index: {sampled_img_idx}')\n",
    "sampled_img_idx = 712344\n",
    "show_imgs([sampled_img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76693726-94d1-4677-8bfb-f3207b48f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10 similar images\n",
    "sim_d = {}\n",
    "sampled_img_v = img_emb[sampled_img_idx]\n",
    "for img_idx, img_v in enumerate(img_emb):\n",
    "    sim_d[img_idx] = cos_sim(sampled_img_v, img_v)\n",
    "\n",
    "sorted_sim = sorted(sim_d.items(), key=lambda x:x[1], reverse=True)\n",
    "top10_imgs = {t[0]: t[1] for t in sorted_sim[1:11]}\n",
    "\n",
    "show_imgs_2_rows(top10_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e3cad-de15-4eba-a082-891f9834cb45",
   "metadata": {},
   "source": [
    "## 2-3. Pick an image and find the most similar images based on K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e630fc6-a56e-449d-8efd-cfba67c85776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33530182-f3e5-4349-9427-ee8591705c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = img_emb.shape[0]\n",
    "num_sampled_imgs = int(0.1 * num_imgs)\n",
    "sampled_imgs = np.random.choice(num_imgs, num_sampled_imgs, replace=False)\n",
    "\n",
    "sampled_img_emb = img_emb[sampled_imgs]\n",
    "sampled_img_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25804a8-3f23-4004-9071-61cd71b98653",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time()\n",
    "kmeans = KMeans(n_clusters=100, random_state=0).fit(sampled_img_emb)\n",
    "toc = time()\n",
    "print(f'K-means fitting: {toc - tic:.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28801ffb-b310-4ce2-9183-71e1366cba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "img_clusters = {}\n",
    "for img_idx, label in enumerate(kmeans_labels):\n",
    "    if label not in img_clusters:\n",
    "        img_clusters[label] = {}\n",
    "    img_clusters[label][img_idx] = True\n",
    "    \n",
    "for label in img_clusters:\n",
    "    img_clusters[label] = list(img_clusters[label].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ce58e-eddb-4923-8062-bc8031409f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_img_clusters = sorted(img_clusters.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "for k, v in sorted_img_clusters:\n",
    "    print(f'Cluster-{k:02d}, {len(v)} imgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156b09e-f2af-4cfe-adce-4907cbf2b803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cluster_label, cluster_imgs in sorted_img_clusters:\n",
    "    np.random.shuffle(cluster_imgs)\n",
    "    show_imgs_2_rows(cluster_imgs[:10], title=f'cluster-{cluster_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5570ff-3762-4d44-96e8-450f50cfb8f7",
   "metadata": {},
   "source": [
    "## 2-4. Pick an image and find the most similar images based on DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821b122-d00e-49b0-9325-aefe827bf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3e58d-8140-4c42-af7d-b784421b4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = img_emb.shape[0]\n",
    "num_sampled_imgs = int(0.1 * num_imgs)\n",
    "sampled_imgs = np.random.choice(num_imgs, num_sampled_imgs, replace=False)\n",
    "\n",
    "sampled_img_emb = img_emb[sampled_imgs]\n",
    "sampled_img_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2249bf-d474-4453-9bc3-a0188656f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time()\n",
    "dbscan = DBSCAN(min_samples=2, eps=0.1).fit(sampled_img_emb)\n",
    "toc = time()\n",
    "print(f'DBSCAN fitting: {toc - tic:.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd085e-32ea-428d-88f9-02acea218d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_labels = dbscan.labels_\n",
    "\n",
    "img_clusters = {}\n",
    "for img_idx, label in enumerate(dbscan_labels):\n",
    "    if label not in img_clusters:\n",
    "        img_clusters[label] = {}\n",
    "    img_clusters[label][img_idx] = True\n",
    "    \n",
    "for label in img_clusters:\n",
    "    img_clusters[label] = list(img_clusters[label].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a3bc1-40f3-43b6-a006-61e1e9152307",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_img_clusters[0][-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f58b5d-66bc-4cb1-9c25-6bdc78f1c1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_imgs(sorted_img_clusters[1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f73c7-f6f3-4b66-998d-efa5da637028",
   "metadata": {},
   "source": [
    "## 2-5. Can we sample pairs quickly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ac9ec-5f66-42b7-8e18-a19f0692bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d81f86-efc3-4ff6-b2d3-dfc045973bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKKeeper:\n",
    "    \"\"\"\n",
    "    Keep k (key, val, content) paris, where they are sorted in the descending \n",
    "    order. If a new inserted pair (key', val', content') has lower value than \n",
    "    the minimum value of the item (i.e., val' < min val), it won't be inserted.\n",
    "    Content is optional.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.keys = []\n",
    "        self.vals = []\n",
    "        self.contents = []\n",
    "\n",
    "\n",
    "    def will_insert(self, val):\n",
    "        if len(self.vals) < self.k:\n",
    "            return True\n",
    "        elif self.vals[-1] < val:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "        \n",
    "    def insert(self, val, key=None, content=None):\n",
    "        # Check whether we want to insert the item or not\n",
    "        if not self.will_insert(val):\n",
    "            return\n",
    "        \n",
    "        # Insert\n",
    "        reach_end = True\n",
    "        for i, e in enumerate(self.vals):\n",
    "            if e < val:\n",
    "                if key is not None:\n",
    "                    self.keys = self.keys[:i] + [key] + self.keys[i:]\n",
    "                    self.keys = self.keys[:self.k]\n",
    "\n",
    "                self.vals = self.vals[:i] + [val] + self.vals[i:]\n",
    "                self.vals = self.vals[:self.k]\n",
    "\n",
    "                if content is not None:\n",
    "                    self.contents = \\\n",
    "                        self.contents[:i] + [content] + self.contents[i:]\n",
    "                    self.contents = self.contents[:self.k]\n",
    "\n",
    "                reach_end = False\n",
    "                break\n",
    "        if reach_end:\n",
    "            if key is not None:\n",
    "                self.keys.append(key)\n",
    "                self.keys = self.keys[:self.k]\n",
    "\n",
    "            self.vals.append(val)\n",
    "            self.vals = self.vals[:self.k]\n",
    "\n",
    "            if content is not None:\n",
    "                self.contents.append(content)\n",
    "                self.contents = self.contents[:self.k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb18b4b-fc31-4042-b6b5-7f1bb494fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigbor_imgs = {}\n",
    "with tqdm(total=len(img_emb)) as pbar:\n",
    "    for img_i, v_i in enumerate(img_emb):\n",
    "        topk_keeper = TopKKeeper(10)\n",
    "        for img_j, v_j in enumerate(img_emb):\n",
    "            if img_i == img_j:\n",
    "                continue\n",
    "            s = cos_sim(v_i, v_j)\n",
    "            topk_keeper.insert(s, key=img_j)\n",
    "        similar_imgs = topk_keeper.keys\n",
    "        neigbor_imgs[img_i] = similar_imgs[:]\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930df714-e13e-4e3a-b810-32596ab0c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_idx in neigbor_imgs:\n",
    "    show_imgs([img_idx])\n",
    "    show_imgs_2_rows(neigbor_imgs[img_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a6508-7697-470b-875f-eaa206ff1b13",
   "metadata": {},
   "source": [
    "# 3. Can image embedding (from layer activation) approximate the similarity between neurons in different models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524963eb-3243-44a4-ae57-8379f77f77e9",
   "metadata": {},
   "source": [
    "# 3-1. Load stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab4bbc-57e7-41ff-81c3-84686d784e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_stimulus_path(model_nickname, topk_s):\n",
    "    return f'/raid/NeuEvo/data/stimulus/{model_nickname}/data/stimulus-topk_s={topk_s}.json'\n",
    "\n",
    "def load_stimulus(model_nickname, topk_s):\n",
    "    p = get_stimulus_path(model_nickname, topk_s)\n",
    "    data = load_json(p)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd877ac2-42d9-4d35-b083-f54ff1cdadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nickname_1 = 'vgg19_pretrained'\n",
    "model_nickname_2 = 'convnext_0.004_96'\n",
    "\n",
    "topk_s = 20\n",
    "\n",
    "stimulus_1 = load_stimulus(model_nickname_1, topk_s)\n",
    "stimulus_2 = load_stimulus(model_nickname_2, topk_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8adecdc-612c-46f0-a195-932af05565af",
   "metadata": {},
   "source": [
    "## 3-2. Sample a neuron from model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526a5db-6be3-4d2b-8213-2ec3969e97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_layer(stimulus):\n",
    "    layers = list(stimulus.keys())\n",
    "    sampled_layer = np.random.choice(layers)\n",
    "    return sampled_layer\n",
    "\n",
    "def sample_neuron(stimulus):\n",
    "    sampled_layer = sample_layer(stimulus)\n",
    "    num_neurons = len(stimulus[sampled_layer])\n",
    "    sampled_neuron = np.random.choice(num_neurons)\n",
    "    neuron_id = f'{sampled_layer}-{sampled_neuron}'\n",
    "    return neuron_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197e969-ad45-4cfa-a6ce-c5dd14a36225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ex_patch_dir_path(model_nickname, topk_s, ex_patch_size_ratio):\n",
    "    root = '/raid/NeuEvo/data/neuron_feature'\n",
    "    return f'{root}/{model_nickname}/data/topk_s={topk_s}-ex_patch_size_ratio={ex_patch_size_ratio}'\n",
    "\n",
    "def get_ex_patch_img_paths(model_nickname, topk_s, ex_patch_size_ratio, layer, neuron_idx):\n",
    "    d = get_ex_patch_dir_path(model_nickname, topk_s, ex_patch_size_ratio)\n",
    "    paths = []\n",
    "    for i in range(topk_s):\n",
    "        paths.append(f'{d}/{layer}-{neuron_idx}-{i}.jpg')\n",
    "    return paths\n",
    "\n",
    "def load_img_from_path(p):\n",
    "    img = cv2.imread(p)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "                     \n",
    "def show_ex_patch(model_nickname, topk_s, ex_patch_size_ratio, layer, neuron_idx):\n",
    "    img_ps = get_ex_patch_img_paths(model_nickname, topk_s, ex_patch_size_ratio, layer, neuron_idx)\n",
    "    imgs = [load_img_from_path(img_path) for img_path in img_ps]\n",
    "    title = f'{model_nickname}, {layer}-{neuron_idx}'\n",
    "    show_np_imgs_2_rows(imgs, title=title, subplot_titles=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21bde1d-739e-4f2b-bb38-e7746baa765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_neuron_id_1 = sample_neuron(stimulus_1)\n",
    "sampled_neuron_id_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b74399-f9c9-41d6-a857-6d0802afdd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_patch_size_ratio = 0.3\n",
    "sampled_neuron_layer, sampled_neuron_idx = sampled_neuron_id_1.split('-')\n",
    "sampled_neuron_idx = int(sampled_neuron_idx)\n",
    "show_ex_patch(model_nickname_1, 15, ex_patch_size_ratio, sampled_neuron_layer, sampled_neuron_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0aba3-02c6-4b4a-bf6a-3350a01ae68a",
   "metadata": {},
   "source": [
    "## 3-3. Find neurons in model 2 that are similar to the sampled neuron in model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727e9fd-bf24-42d1-9ac4-81949f3d4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKKeeper:\n",
    "    \"\"\"\n",
    "    Keep k (key, val, content) paris, where they are sorted in the descending \n",
    "    order. If a new inserted pair (key', val', content') has lower value than \n",
    "    the minimum value of the item (i.e., val' < min val), it won't be inserted.\n",
    "    Content is optional.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.keys = []\n",
    "        self.vals = []\n",
    "        self.contents = []\n",
    "\n",
    "\n",
    "    def will_insert(self, val):\n",
    "        if len(self.vals) < self.k:\n",
    "            return True\n",
    "        elif self.vals[-1] < val:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "        \n",
    "    def insert(self, val, key=None, content=None):\n",
    "        # Check whether we want to insert the item or not\n",
    "        if not self.will_insert(val):\n",
    "            return\n",
    "        \n",
    "        # Insert\n",
    "        reach_end = True\n",
    "        for i, e in enumerate(self.vals):\n",
    "            if e < val:\n",
    "                if key is not None:\n",
    "                    self.keys = self.keys[:i] + [key] + self.keys[i:]\n",
    "                    self.keys = self.keys[:self.k]\n",
    "\n",
    "                self.vals = self.vals[:i] + [val] + self.vals[i:]\n",
    "                self.vals = self.vals[:self.k]\n",
    "\n",
    "                if content is not None:\n",
    "                    self.contents = \\\n",
    "                        self.contents[:i] + [content] + self.contents[i:]\n",
    "                    self.contents = self.contents[:self.k]\n",
    "\n",
    "                reach_end = False\n",
    "                break\n",
    "        if reach_end:\n",
    "            if key is not None:\n",
    "                self.keys.append(key)\n",
    "                self.keys = self.keys[:self.k]\n",
    "\n",
    "            self.vals.append(val)\n",
    "            self.vals = self.vals[:self.k]\n",
    "\n",
    "            if content is not None:\n",
    "                self.contents.append(content)\n",
    "                self.contents = self.contents[:self.k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cbbfc-c0c7-4f27-88f2-cc0f76523164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_img_emb(stimuli, dim, img_emb):\n",
    "    avg_v = np.zeros(dim)\n",
    "    for img in stimuli:\n",
    "        v = img_emb[img]\n",
    "        avg_v += v\n",
    "    avg_v = avg_v / len(stimuli)\n",
    "    return avg_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc351888-da93-46a4-a3eb-dacb5373ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_neuron_stimuli = stimulus_1[sampled_neuron_layer][sampled_neuron_idx]\n",
    "sampled_neuron_v = get_avg_img_emb(sampled_neuron_stimuli, dim, img_emb)\n",
    "\n",
    "dim = 30\n",
    "topk_keeper = TopKKeeper(10)\n",
    "for layer in stimulus_2:\n",
    "    for neuron_idx, neuron_stimuli in enumerate(stimulus_2[layer]):\n",
    "        avg_v = get_avg_img_emb(neuron_stimuli, dim, img_emb)\n",
    "        sim = cos_sim(sampled_neuron_v, avg_v)\n",
    "        topk_keeper.insert(sim, key=f'{layer}-{neuron_idx}')\n",
    "        \n",
    "print(topk_keeper.keys)\n",
    "print(topk_keeper.vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb230b8-f8c6-48bc-8933-a550a1bfef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_neurons = topk_keeper.keys\n",
    "topk_s = 20\n",
    "for neuron in similar_neurons:\n",
    "    layer, idx = neuron.split('-')\n",
    "    idx = int(idx)\n",
    "    show_ex_patch(\n",
    "        model_nickname_2, \n",
    "        topk_s, \n",
    "        ex_patch_size_ratio, \n",
    "        layer, \n",
    "        idx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb942b-d09d-4a50-a6e7-7ea41ce48425",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in stimulus_1.keys():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790796a-1887-41b4-b51c-b8bf2046b639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concept-evolution",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
